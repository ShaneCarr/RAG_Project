Proposal for structure:

Part I – Math Foundations

Chapter 1: Vectors and Linear Algebra

Chapter 2: Matrices, multiplication, eigenstuff (only as much as needed)

Chapter 3: Calculus for ML (derivatives, chain rule, gradients)

Part II – Neural Networks

Chapter 4: The Neuron and Forward Propagation

Chapter 5: Loss functions and Optimization

Chapter 6: Backpropagation (step-by-step example with actual numbers)

Part III – Representations

Chapter 7: Word2Vec and embeddings

Chapter 8: Transformers explained (attention, positional encoding)

Chapter 9: How embeddings are trained today (contrastive learning)

Part IV – Retrieval

Chapter 10: Cosine similarity and distance metrics

Chapter 11: kNN and Approximate Nearest Neighbor search

Chapter 12: Vector databases (FAISS, Pinecone, etc.)

Part V – RAG

Chapter 13: Putting it all together (repo → embeddings → search → LLM)

Chapter 14: End-to-end worked example with code snippets

Chapter 15: Where this is going (real-time updates, hybrid search, multimodal)
